{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Define paths to data\n",
    "root_dir = r\"C:\\Users\\avs20\\Documents\\GitHub\\FacialExpressionAI_Tanaka2023\\Squeezenet_FullImage\\data\"  # Replace with the root directory containing 'pain', 'tickle', and 'normal' folders\n",
    "\n",
    "# Define transformations with data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "# Split the dataset (64:16:20)\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.64 * total_size)\n",
    "val_size = int(0.16 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load the pre-trained SqueezeNet model and fine-tune\n",
    "model = models.squeezenet1_1(pretrained=True)\n",
    "model.classifier[1] = nn.Conv2d(512, 3, kernel_size=1)  # Modify for 3 classes\n",
    "model.num_classes = 3\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-4, momentum=0.9)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "val_interval = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation at intervals\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Testing the model\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Confusion matrix, sensitivity, and specificity\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "sensitivity = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
    "specificity = (conf_matrix.sum() - conf_matrix.sum(axis=0) - conf_matrix.sum(axis=1) + conf_matrix.diagonal()) / (conf_matrix.sum() - conf_matrix.sum(axis=1))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"Sensitivity: {sensitivity}\")\n",
    "print(f\"Specificity: {specificity}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define paths to data\n",
    "root_dir = r\"C:\\Users\\avs20\\Documents\\GitHub\\FacialExpressionAI_Tanaka2023\\Squeezenet_FullImage\\data\"  # Replace with your dataset path\n",
    "\n",
    "# Define transformations with data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "# Split the dataset (64:16:20)\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.64 * total_size)\n",
    "val_size = int(0.16 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load the pre-trained SqueezeNet model and fine-tune\n",
    "model = models.squeezenet1_1(pretrained=True)\n",
    "model.classifier[1] = nn.Conv2d(512, 3, kernel_size=1)  # Modify for 3 classes\n",
    "model.num_classes = 3\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-4, momentum=0.9)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Hook to extract feature vectors\n",
    "def extract_features(images):\n",
    "    with torch.no_grad():\n",
    "        # Pass images through the feature extractor\n",
    "        features = model.features(images)\n",
    "        # Global Average Pooling to reduce dimensions\n",
    "        features = nn.functional.adaptive_avg_pool2d(features, (1, 1)).squeeze(-1).squeeze(-1)\n",
    "    return features\n",
    "\n",
    "# Function to save feature vectors to a CSV file\n",
    "def save_features_to_csv(data, file_name):\n",
    "    df = pd.DataFrame(data, columns=[\"Image_Name\", \"Class\", \"Feature_Vector\"])\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 110\n",
    "val_interval = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if epoch == num_epochs - 1:  # To store features during the final training epoch\n",
    "        final_train_features = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Store feature vectors in the final epoch\n",
    "        if epoch == num_epochs - 1:\n",
    "            features = extract_features(inputs).cpu().numpy()\n",
    "            image_names = [dataset.samples[idx][0] for idx in range(len(dataset.samples))]\n",
    "            for name, label, feature in zip(image_names, labels.cpu().numpy(), features):\n",
    "                final_train_features.append([name, label, feature.tolist()])\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    # Validation at intervals\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save final training features\n",
    "if final_train_features:\n",
    "    save_features_to_csv(final_train_features, \"final_train_features.csv\")\n",
    "\n",
    "# Testing the model and extracting features\n",
    "model.eval()\n",
    "test_features = []\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Collect predictions and labels\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Extract and save feature vectors for test data\n",
    "        features = extract_features(inputs).cpu().numpy()\n",
    "        image_names = [dataset.samples[idx][0] for idx in range(len(dataset.samples))]\n",
    "        for name, label, feature in zip(image_names, labels.cpu().numpy(), features):\n",
    "            test_features.append([name, label, feature.tolist()])\n",
    "\n",
    "# Save test features\n",
    "save_features_to_csv(test_features, \"test_features.csv\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"squeezenet_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Load the model architecture\n",
    "model = models.squeezenet1_1(pretrained=False)  # Use pretrained=False when loading a saved model\n",
    "model.classifier[1] = nn.Conv2d(512, 3, kernel_size=1)  # Ensure the same architecture\n",
    "model.num_classes = 3\n",
    "\n",
    "# Load the saved state dict\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"squeezenet_model.pth\", map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = r'C:\\Users\\avs20\\Documents\\GitHub\\FacialExpressionAI_Tanaka2023\\Squeezenet_FullImage\\data\\examples'\n",
    "# img_name = 'collies.JPG'\n",
    "# img_name = 'multiple_dogs.jpg'\n",
    "# img_name = 'snake.JPEG'\n",
    "img_name = 'pain_0004.bmp'\n",
    "img_path = os.path.join(img_dir, img_name)\n",
    "\n",
    "pil_img = PIL.Image.open(img_path)\n",
    "pil_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "torch_img = torch.from_numpy(np.asarray(pil_img)).permute(2, 0, 1).unsqueeze(0).float().div(255).cuda()\n",
    "torch_img = F.upsample(torch_img, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "normed_torch_img = normalizer(torch_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#squeezenet = models.squeezenet1_1(pretrained=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Load the model architecture\n",
    "squeezenet = models.squeezenet1_1(pretrained=False)  # Use pretrained=False when loading a saved model\n",
    "squeezenet.classifier[1] = nn.Conv2d(512, 3, kernel_size=1)  # Ensure the same architecture\n",
    "squeezenet.num_classes = 3\n",
    "\n",
    "# Load the saved state dict\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "squeezenet.load_state_dict(torch.load(\"squeezenet_model.pth\", map_location=device))\n",
    "squeezenet = squeezenet.to(device)\n",
    "squeezenet.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "squeezenet.eval(), squeezenet.cuda();\n",
    "\n",
    "\n",
    "cam_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet_model_dict = dict(type='squeezenet', arch=squeezenet, layer_name='features_12_expand3x3_activation', input_size=(224, 224))\n",
    "squeezenet_gradcam = GradCAM(squeezenet_model_dict, True)\n",
    "squeezenet_gradcampp = GradCAMpp(squeezenet_model_dict, True)\n",
    "cam_dict['squeezenet'] = [squeezenet_gradcam, squeezenet_gradcampp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for gradcam, gradcam_pp in cam_dict.values():\n",
    "    mask, _ = gradcam(normed_torch_img)\n",
    "    heatmap, result = visualize_cam(mask, torch_img)\n",
    "\n",
    "    mask_pp, _ = gradcam_pp(normed_torch_img)\n",
    "    heatmap_pp, result_pp = visualize_cam(mask_pp, torch_img)\n",
    "    \n",
    "    images.append(torch.stack([torch_img.squeeze().cpu(), heatmap, heatmap_pp, result, result_pp], 0))\n",
    "    \n",
    "images = make_grid(torch.cat(images, 0), nrow=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_name = img_name\n",
    "output_path = os.path.join(output_dir, output_name)\n",
    "\n",
    "save_image(images, output_path)\n",
    "PIL.Image.open(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torchvision import models\n",
    "from torchvision.transforms import Normalize\n",
    "import PIL.Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils import visualize_cam, Normalize\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "import cv2\n",
    "\n",
    "# Define input and output directories\n",
    "img_dir = r'C:\\Users\\avs20\\Documents\\GitHub\\FacialExpressionAI_Tanaka2023\\Squeezenet_FullImage\\data\\examples'\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the model\n",
    "squeezenet = models.squeezenet1_1(pretrained=False)\n",
    "squeezenet.classifier[1] = nn.Conv2d(512, 3, kernel_size=1)\n",
    "squeezenet.num_classes = 3\n",
    "\n",
    "# Load the saved model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "squeezenet.load_state_dict(torch.load(\"squeezenet_model.pth\", map_location=device))\n",
    "squeezenet = squeezenet.to(device)\n",
    "squeezenet.eval()\n",
    "\n",
    "# Initialize GradCAM and GradCAM++\n",
    "squeezenet_model_dict = dict(\n",
    "    type='squeezenet',\n",
    "    arch=squeezenet,\n",
    "    layer_name='features_12_expand3x3_activation',\n",
    "    input_size=(224, 224),\n",
    ")\n",
    "gradcam = GradCAM(squeezenet_model_dict, True)\n",
    "gradcam_pp = GradCAMpp(squeezenet_model_dict, True)\n",
    "cam_dict = {'Grad-CAM': gradcam, 'Grad-CAM++': gradcam_pp}\n",
    "\n",
    "# Define normalizer\n",
    "normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Process all images in img_dir\n",
    "for img_name in os.listdir(img_dir):\n",
    "    img_path = os.path.join(img_dir, img_name)\n",
    "    if not img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
    "        continue  # Skip non-image files\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    pil_img = PIL.Image.open(img_path).convert(\"RGB\")\n",
    "    torch_img = torch.from_numpy(np.asarray(pil_img)).permute(2, 0, 1).unsqueeze(0).float().div(255).to(device)\n",
    "    torch_img = F.interpolate(torch_img, size=(224, 224), mode='bilinear', align_corners=False) #try hashing this out to avoid images becoming squares\n",
    "    normed_torch_img = normalizer(torch_img)\n",
    "\n",
    "    # Generate Grad-CAM and Grad-CAM++ heatmaps\n",
    "    images = [torch_img.squeeze().cpu()]  # Start with the original image\n",
    "    titles = [f\"Original Image: {img_name}\"]  # Add the original image title\n",
    "\n",
    "    for cam_name, cam_method in cam_dict.items():\n",
    "        mask, _ = cam_method(normed_torch_img)\n",
    "        heatmap, result = visualize_cam(mask, torch_img)\n",
    "\n",
    "        # Collect images and titles for plotting\n",
    "        images.extend([heatmap, result])\n",
    "        titles.extend([f\"{cam_name} Heatmap\", f\"{cam_name} Result\"])\n",
    "\n",
    "    # Plot and save the results\n",
    "    fig, axs = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "    for i, img in enumerate(images):\n",
    "        axs[i].imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "        axs[i].set_title(titles[i], fontsize=10)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(output_dir, f\"{os.path.splitext(img_name)[0]}_heatmaps.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avs20\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\avs20\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\avs20\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1352: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "c:\\Users\\avs20\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3782: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saliency_map size : torch.Size([8, 14])\n",
      "saliency_map size : torch.Size([8, 14])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "#Code with square pictures\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import PIL.Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#from utils import visualize_cam\n",
    "from viz_cam_updated2 import visualize_cam\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Define input and output directories\n",
    "img_dir = r'C:\\Users\\avs20\\Documents\\GitHub\\FacialExpressionAI_Tanaka2023\\Squeezenet_FullImage\\data\\examples'\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_paths = [\n",
    "            os.path.join(img_dir, img_name)\n",
    "            for img_name in os.listdir(img_dir)\n",
    "            if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = PIL.Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, os.path.basename(img_path)  # Return image and its file name\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load dataset and dataloader\n",
    "dataset = ImageDataset(img_dir=img_dir, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "squeezenet = models.squeezenet1_1(pretrained=False)\n",
    "squeezenet.classifier[1] = nn.Conv2d(512, 3, kernel_size=1)\n",
    "squeezenet.num_classes = 3\n",
    "\n",
    "# Load the saved model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "squeezenet.load_state_dict(torch.load(\"squeezenet_model.pth\", map_location=device))\n",
    "squeezenet = squeezenet.to(device)\n",
    "squeezenet.eval()\n",
    "\n",
    "# Initialize GradCAM and GradCAM++\n",
    "dummy_input = next(iter(data_loader))[0][0]  # Get a single sample to determine dimensions\n",
    "input_size = dummy_input.shape[1:]  # Extract height and width\n",
    "\n",
    "squeezenet_model_dict = dict(\n",
    "    type='squeezenet',\n",
    "    arch=squeezenet,\n",
    "    layer_name='features_12_expand3x3_activation',\n",
    "    input_size=input_size,\n",
    ")\n",
    "gradcam = GradCAM(squeezenet_model_dict, True)\n",
    "gradcam_pp = GradCAMpp(squeezenet_model_dict, True)\n",
    "cam_dict = {'Grad-CAM': gradcam, 'Grad-CAM++': gradcam_pp}\n",
    "\n",
    "# Process all images in the dataset\n",
    "for images, img_names in data_loader:\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Normalize input image\n",
    "    normed_images = images.clone()  # Normalization already applied in transform\n",
    "\n",
    "    # Generate Grad-CAM and Grad-CAM++ heatmaps\n",
    "    images = [images[0].cpu()]  # Start with the original image\n",
    "    titles = [f\"Original Image: {img_names[0]}\"]  # Add the original image title\n",
    "\n",
    "    for cam_name, cam_method in cam_dict.items():\n",
    "        mask, _ = cam_method(normed_images)\n",
    "        heatmap, result = visualize_cam(mask, images[0])\n",
    "\n",
    "        # Collect images and titles for plotting\n",
    "        images.extend([heatmap, result])\n",
    "        titles.extend([f\"{cam_name} Heatmap\", f\"{cam_name} Result\"])\n",
    "\n",
    "    # Plot and save the results\n",
    "    fig, axs = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "    for i, img in enumerate(images):\n",
    "        axs[i].imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "        axs[i].set_title(titles[i], fontsize=10)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(output_dir, f\"{os.path.splitext(img_names[0])[0]}_heatmaps.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saliency_map size : torch.Size([8, 14])\n",
      "saliency_map size : torch.Size([8, 14])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import PIL.Image\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from viz_cam_updated2 import visualize_cam\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "\n",
    "# Define input and output directories\n",
    "img_dir = r'C:\\Users\\avs20\\Documents\\GitHub\\FacialExpressionAI_Tanaka2023\\Squeezenet_FullImage\\data\\examples'\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_paths = [\n",
    "            os.path.join(img_dir, img_name)\n",
    "            for img_name in os.listdir(img_dir)\n",
    "            if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = PIL.Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, os.path.basename(img_path)  # Return image and its file name\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Model normalization\n",
    "])\n",
    "\n",
    "# Load dataset and dataloader\n",
    "dataset = ImageDataset(img_dir=img_dir, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "squeezenet = models.squeezenet1_1(pretrained=False)\n",
    "squeezenet.classifier[1] = nn.Conv2d(512, 3, kernel_size=1)\n",
    "squeezenet.num_classes = 3\n",
    "\n",
    "# Load the saved model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "squeezenet.load_state_dict(torch.load(\"squeezenet_model.pth\", map_location=device))\n",
    "squeezenet = squeezenet.to(device)\n",
    "squeezenet.eval()\n",
    "\n",
    "# Initialize GradCAM and GradCAM++\n",
    "dummy_input = next(iter(data_loader))[0][0]  # Get a single sample to determine dimensions\n",
    "input_size = dummy_input.shape[1:]  # Extract height and width\n",
    "\n",
    "squeezenet_model_dict = dict(\n",
    "    type='squeezenet',\n",
    "    arch=squeezenet,\n",
    "    layer_name='features_12_expand3x3_activation',\n",
    "    input_size=input_size,\n",
    ")\n",
    "gradcam = GradCAM(squeezenet_model_dict, True)\n",
    "gradcam_pp = GradCAMpp(squeezenet_model_dict, True)\n",
    "cam_dict = {'Grad-CAM': gradcam, 'Grad-CAM++': gradcam_pp}\n",
    "\n",
    "# Unnormalize function for displaying the original image\n",
    "def unnormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return tensor * std + mean  # Reverse the normalization\n",
    "\n",
    "# Process all images in the dataset\n",
    "for images, img_names in data_loader:\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Normalize input image\n",
    "    normed_images = images.clone()  # Normalization already applied in transform\n",
    "\n",
    "    # Unnormalize the original image before displaying\n",
    "    unnormed_image = unnormalize(images[0].cpu()).clamp(0, 1)  # Ensure valid range [0,1]\n",
    "    images = [unnormed_image]  # Use unnormalized image instead\n",
    "\n",
    "    titles = [f\"Original Image: {img_names[0]}\"]  # Add the original image title\n",
    "\n",
    "    for cam_name, cam_method in cam_dict.items():\n",
    "        mask, _ = cam_method(normed_images)\n",
    "        heatmap, result = visualize_cam(mask, images[0])\n",
    "\n",
    "        # Collect images and titles for plotting\n",
    "        images.extend([heatmap, result])\n",
    "        titles.extend([f\"{cam_name} Heatmap\", f\"{cam_name} Result\"])\n",
    "\n",
    "    # Plot and save the results\n",
    "    fig, axs = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "    for i, img in enumerate(images):\n",
    "        axs[i].imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "        axs[i].set_title(titles[i], fontsize=10)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(output_dir, f\"{os.path.splitext(img_names[0])[0]}_heatmaps.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
