{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define paths to data\n",
    "root_dir = r\"C:\\Users\\avs20\\Documents\\GitHub\\FacialExpressionAI_Tanaka2023\\Squeezenet_FullImage\\data2emotions\"  # Replace with your dataset path\n",
    "\n",
    "# Define transformations with data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(root=root_dir, transform=transform)\n",
    "\n",
    "# Split the dataset (64:16:20)\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.64 * total_size)\n",
    "val_size = int(0.16 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load the pre-trained SqueezeNet model and fine-tune\n",
    "model = models.squeezenet1_1(pretrained=True)\n",
    "model.classifier[1] = nn.Conv2d(512, 2, kernel_size=1)  # Modify for 2 classes\n",
    "model.num_classes = 2\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-4, momentum=0.9)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Hook to extract feature vectors\n",
    "def extract_features(images):\n",
    "    with torch.no_grad():\n",
    "        # Pass images through the feature extractor\n",
    "        features = model.features(images)\n",
    "        # Global Average Pooling to reduce dimensions\n",
    "        features = nn.functional.adaptive_avg_pool2d(features, (1, 1)).squeeze(-1).squeeze(-1)\n",
    "    return features\n",
    "\n",
    "# Function to save feature vectors to a CSV file\n",
    "def save_features_to_csv(data, file_name):\n",
    "    df = pd.DataFrame(data, columns=[\"Image_Name\", \"Class\", \"Feature_Vector\"])\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 110\n",
    "val_interval = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if epoch == num_epochs - 1:  # To store features during the final training epoch\n",
    "        final_train_features = []\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Store feature vectors in the final epoch\n",
    "        if epoch == num_epochs - 1:\n",
    "            features = extract_features(inputs).cpu().numpy()\n",
    "            image_names = [dataset.samples[idx][0] for idx in range(len(dataset.samples))]\n",
    "            for name, label, feature in zip(image_names, labels.cpu().numpy(), features):\n",
    "                final_train_features.append([name, label, feature.tolist()])\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    # Validation at intervals\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save final training features\n",
    "if final_train_features:\n",
    "    save_features_to_csv(final_train_features, \"final_train_features.csv\")\n",
    "\n",
    "# Testing the model and extracting features\n",
    "model.eval()\n",
    "test_features = []\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        # Collect predictions and labels\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Extract and save feature vectors for test data\n",
    "        features = extract_features(inputs).cpu().numpy()\n",
    "        image_names = [dataset.samples[idx][0] for idx in range(len(dataset.samples))]\n",
    "        for name, label, feature in zip(image_names, labels.cpu().numpy(), features):\n",
    "            test_features.append([name, label, feature.tolist()])\n",
    "\n",
    "# Save test features\n",
    "save_features_to_csv(test_features, \"test_features.csv\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"squeezenet_model_2emotions.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.class_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\avs20\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\avs20\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.8740, Accuracy: 61.00%\n",
      "Validation Loss: 0.5999, Accuracy: 66.67%\n",
      "Epoch 2/10, Loss: 0.5592, Accuracy: 68.05%\n",
      "Validation Loss: 0.5028, Accuracy: 66.67%\n",
      "Epoch 3/10, Loss: 0.4578, Accuracy: 78.42%\n",
      "Validation Loss: 0.3651, Accuracy: 96.67%\n",
      "Epoch 4/10, Loss: 0.3206, Accuracy: 93.78%\n",
      "Validation Loss: 0.1977, Accuracy: 98.33%\n",
      "Epoch 5/10, Loss: 0.1518, Accuracy: 98.76%\n",
      "Validation Loss: 0.0670, Accuracy: 100.00%\n",
      "Epoch 6/10, Loss: 0.0528, Accuracy: 100.00%\n",
      "Validation Loss: 0.0236, Accuracy: 100.00%\n",
      "Epoch 7/10, Loss: 0.0196, Accuracy: 100.00%\n",
      "Validation Loss: 0.0081, Accuracy: 100.00%\n",
      "Epoch 8/10, Loss: 0.0115, Accuracy: 100.00%\n",
      "Validation Loss: 0.0162, Accuracy: 100.00%\n",
      "Epoch 9/10, Loss: 0.0103, Accuracy: 100.00%\n",
      "Validation Loss: 0.0043, Accuracy: 100.00%\n",
      "Epoch 10/10, Loss: 0.0047, Accuracy: 100.00%\n",
      "Validation Loss: 0.0030, Accuracy: 100.00%\n",
      "Confusion Matrix:\n",
      "[[48  0]\n",
      " [ 0 29]]\n"
     ]
    }
   ],
   "source": [
    "# use pretrained model on Tanaka, fine tune on gogolla\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models  # Import models here\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Define paths to the new dataset\n",
    "new_root_dir = r\"C:\\Users\\avs20\\Documents\\GitHub\\FacialExpressionAI_Tanaka2023\\Squeezenet_FullImage\\data2emotionsGogolla\"  # Replace with the new dataset path\n",
    "\n",
    "# Define transformations for the new dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the new dataset\n",
    "new_dataset = datasets.ImageFolder(root=new_root_dir, transform=transform)\n",
    "\n",
    "# Split the new dataset (64:16:20)\n",
    "total_size = len(new_dataset)\n",
    "train_size = int(0.64 * total_size)\n",
    "val_size = int(0.16 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "new_train_dataset, new_val_dataset, new_test_dataset = random_split(new_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create data loaders for the new dataset\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(new_val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(new_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load the pretrained model\n",
    "# Load the pretrained model\n",
    "model = models.squeezenet1_1(pretrained=False)  # Initialize the same architecture as the pretrained model\n",
    "model.classifier[1] = nn.Conv2d(512, 2, kernel_size=1)  # Modify for 2 classes\n",
    "model.num_classes = 2\n",
    "pretrained_model_path = \"squeezenet_model_2emotions.pth\"  # Path to the saved state_dict\n",
    "state_dict = torch.load(pretrained_model_path)  # Load the state_dict\n",
    "model.load_state_dict(state_dict)  # Load the weights into the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)  # Smaller learning rate for fine-tuning\n",
    "\n",
    "# Fine-tuning loop\n",
    "num_epochs = 10\n",
    "val_interval = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "    # Validation at intervals\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "torch.save(model, \"squeezenet_fine_tuned_on_new_dataset.pth\")\n",
    "\n",
    "# Evaluate on the test dataset\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradCAM predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import PIL.Image\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from viz_cam_updated2 import visualize_cam\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "\n",
    "# Define input and output directories\n",
    "img_dir = r'C:\\Users\\avs20\\Documents\\GitHub\\FacialExpressionAI_Tanaka2023\\Squeezenet_FullImage\\data2emotions\\examples'\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_paths = [\n",
    "            os.path.join(img_dir, img_name)\n",
    "            for img_name in os.listdir(img_dir)\n",
    "            if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = PIL.Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, os.path.basename(img_path)  # Return image and its file name\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Model normalization\n",
    "])\n",
    "\n",
    "# Load dataset and dataloader\n",
    "dataset = ImageDataset(img_dir=img_dir, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "squeezenet = models.squeezenet1_1(pretrained=False)\n",
    "squeezenet.classifier[1] = nn.Conv2d(512, 2, kernel_size=1)\n",
    "squeezenet.num_classes = 2\n",
    "\n",
    "# Load the saved model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "squeezenet.load_state_dict(torch.load(\"squeezenet_model_2emotions.pth\", map_location=device))\n",
    "squeezenet = squeezenet.to(device)\n",
    "squeezenet.eval()\n",
    "\n",
    "# Initialize GradCAM and GradCAM++\n",
    "dummy_input = next(iter(data_loader))[0][0]  # Get a single sample to determine dimensions\n",
    "input_size = dummy_input.shape[1:]  # Extract height and width\n",
    "\n",
    "squeezenet_model_dict = dict(\n",
    "    type='squeezenet',\n",
    "    arch=squeezenet,\n",
    "    layer_name='features_12_expand3x3_activation',\n",
    "    input_size=input_size,\n",
    ")\n",
    "gradcam = GradCAM(squeezenet_model_dict, True)\n",
    "gradcam_pp = GradCAMpp(squeezenet_model_dict, True)\n",
    "cam_dict = {'Grad-CAM': gradcam, 'Grad-CAM++': gradcam_pp}\n",
    "\n",
    "# Unnormalize function for displaying the original image\n",
    "def unnormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return tensor * std + mean  # Reverse the normalization\n",
    "\n",
    "# Process all images in the dataset\n",
    "for images, img_names in data_loader:\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Normalize input image\n",
    "    normed_images = images.clone()  # Normalization already applied in transform\n",
    "\n",
    "    # Unnormalize the original image before displaying\n",
    "    unnormed_image = unnormalize(images[0].cpu()).clamp(0, 1)  # Ensure valid range [0,1]\n",
    "    images = [unnormed_image]  # Use unnormalized image instead\n",
    "\n",
    "    titles = [f\"Original Image: {img_names[0]}\"]  # Add the original image title\n",
    "\n",
    "    for cam_name, cam_method in cam_dict.items():\n",
    "        mask, _ = cam_method(normed_images)\n",
    "        heatmap, result = visualize_cam(mask, images[0])\n",
    "\n",
    "        # Collect images and titles for plotting\n",
    "        images.extend([heatmap, result])\n",
    "        titles.extend([f\"{cam_name} Heatmap\", f\"{cam_name} Result\"])\n",
    "\n",
    "    # Plot and save the results\n",
    "    fig, axs = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "    for i, img in enumerate(images):\n",
    "        axs[i].imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "        axs[i].set_title(titles[i], fontsize=10)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(output_dir, f\"{os.path.splitext(img_names[0])[0]}_heatmaps.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saliency_map size : torch.Size([47, 91])\n",
      "saliency_map size : torch.Size([47, 91])\n"
     ]
    }
   ],
   "source": [
    "# heatmap generation Gogolla\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torchvision.transforms import Normalize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import PIL.Image\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from viz_cam_updated2 import visualize_cam\n",
    "from gradcam import GradCAM, GradCAMpp\n",
    "\n",
    "# Define input and output directories\n",
    "img_dir = r'C:\\Users\\avs20\\Documents\\GitHub\\FacialExpressionAI_Tanaka2023\\Squeezenet_FullImage\\data2emotionsGogolla\\examples'\n",
    "output_dir = 'outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_paths = [\n",
    "            os.path.join(img_dir, img_name)\n",
    "            for img_name in os.listdir(img_dir)\n",
    "            if img_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        img = PIL.Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, os.path.basename(img_path)  # Return image and its file name\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Model normalization\n",
    "])\n",
    "\n",
    "# Load dataset and dataloader\n",
    "dataset = ImageDataset(img_dir=img_dir, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Initialize the model\n",
    "squeezenet = models.squeezenet1_1(pretrained=False)\n",
    "squeezenet.classifier[1] = nn.Conv2d(512, 2, kernel_size=1)\n",
    "squeezenet.num_classes = 2\n",
    "\n",
    "# Load the saved model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "squeezenet.load_state_dict(torch.load(\"squeezenet_model_2emotions.pth\", map_location=device))\n",
    "squeezenet = squeezenet.to(device)\n",
    "squeezenet.eval()\n",
    "\n",
    "# Initialize GradCAM and GradCAM++\n",
    "dummy_input = next(iter(data_loader))[0][0]  # Get a single sample to determine dimensions\n",
    "input_size = dummy_input.shape[1:]  # Extract height and width\n",
    "\n",
    "squeezenet_model_dict = dict(\n",
    "    type='squeezenet',\n",
    "    arch=squeezenet,\n",
    "    layer_name='features_12_expand3x3_activation',\n",
    "    input_size=input_size,\n",
    ")\n",
    "gradcam = GradCAM(squeezenet_model_dict, True)\n",
    "gradcam_pp = GradCAMpp(squeezenet_model_dict, True)\n",
    "cam_dict = {'Grad-CAM': gradcam, 'Grad-CAM++': gradcam_pp}\n",
    "\n",
    "# Unnormalize function for displaying the original image\n",
    "def unnormalize(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    mean = torch.tensor(mean).view(3, 1, 1)\n",
    "    std = torch.tensor(std).view(3, 1, 1)\n",
    "    return tensor * std + mean  # Reverse the normalization\n",
    "\n",
    "# Process all images in the dataset\n",
    "for images, img_names in data_loader:\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Normalize input image\n",
    "    normed_images = images.clone()  # Normalization already applied in transform\n",
    "\n",
    "    # Unnormalize the original image before displaying\n",
    "    unnormed_image = unnormalize(images[0].cpu()).clamp(0, 1)  # Ensure valid range [0,1]\n",
    "    images = [unnormed_image]  # Use unnormalized image instead\n",
    "\n",
    "    titles = [f\"Original Image: {img_names[0]}\"]  # Add the original image title\n",
    "\n",
    "    for cam_name, cam_method in cam_dict.items():\n",
    "        mask, _ = cam_method(normed_images)\n",
    "        heatmap, result = visualize_cam(mask, images[0])\n",
    "\n",
    "        # Collect images and titles for plotting\n",
    "        images.extend([heatmap, result])\n",
    "        titles.extend([f\"{cam_name} Heatmap\", f\"{cam_name} Result\"])\n",
    "\n",
    "    # Plot and save the results\n",
    "    fig, axs = plt.subplots(1, len(images), figsize=(15, 5))\n",
    "    for i, img in enumerate(images):\n",
    "        axs[i].imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "        axs[i].set_title(titles[i], fontsize=10)\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    output_path = os.path.join(output_dir, f\"{os.path.splitext(img_names[0])[0]}_heatmaps.png\")\n",
    "    plt.savefig(output_path)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
